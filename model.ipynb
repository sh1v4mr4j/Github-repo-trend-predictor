{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/shivamraj/anaconda3/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: filelock in /Users/shivamraj/anaconda3/lib/python3.11/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/shivamraj/anaconda3/lib/python3.11/site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in /Users/shivamraj/anaconda3/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/shivamraj/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/shivamraj/anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/shivamraj/anaconda3/lib/python3.11/site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/shivamraj/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/shivamraj/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   OpenIssuesCount  ForksCount  StargazersCount    Size  description_length\n",
      "0               43       27855           331707    1525                  53\n",
      "1              377       33776           316814    5030                  30\n",
      "2              338       28727           306876    1058                  73\n",
      "3               55       76810           306451   22318                  69\n",
      "4               41       39078           296377  243999                 100\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('most_popular_repositories_1000.csv')\n",
    "\n",
    "dataset['Description'] = dataset['Description'].fillna('')\n",
    "dataset['description_length'] = dataset['Description'].apply(len)\n",
    "\n",
    "columns_to_drop = ['Name', 'FullName', 'HtmlUrl', 'Description', 'Language','OwnerLogin', 'OwnerType', 'CreatedAt', 'UpdatedAt', 'PushedAt', 'License','WatchersCount']\n",
    "\n",
    "dataset = dataset.drop(columns=columns_to_drop)\n",
    "\n",
    "ids = dataset['Id']\n",
    "dataset = dataset.drop(columns = ['Id'])\n",
    "\n",
    "\n",
    "\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitoring directory: ./kafka_streaming/csv_data for new CSV files...\n",
      "New CSV detected: /Users/shivamraj/Desktop/532/532_project/Github-repo-trend-predictor/kafka_streaming/csv_data/stream_2024-12-07_01-46-09.csv\n",
      "Processing CSV file ...\n",
      "pre-processing the data...\n",
      "Processing to get unique 400 data...\n",
      "extracting from train.csv...\n",
      "Length of train.csv 507\n",
      "Train.csv has reached its maximum capacity of 400 unique rows.\n",
      "Creating validation dataset...\n",
      "length of validation.csv 200\n",
      "Validation.csv has reached its maximum capacity of 100 unique rows.\n",
      "Getting training data and validation data and normalising...\n",
      "Combine the data for normalization...\n",
      "Normalising...\n",
      "Training the model...\n",
      "Epoch [100/1000], Train Loss: 0.0245, Val Loss: 0.0292\n",
      "Epoch [200/1000], Train Loss: 0.0182, Val Loss: 0.0233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8r/vk9p_0jn559ggvy1pwy_gczm0000gn/T/ipykernel_28256/3959224966.py:255: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
      "/var/folders/8r/vk9p_0jn559ggvy1pwy_gczm0000gn/T/ipykernel_28256/3959224966.py:269: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/1000], Train Loss: 0.0146, Val Loss: 0.0197\n",
      "Epoch [400/1000], Train Loss: 0.0109, Val Loss: 0.0160\n",
      "Epoch [500/1000], Train Loss: 0.0084, Val Loss: 0.0136\n",
      "Epoch [600/1000], Train Loss: 0.0069, Val Loss: 0.0124\n",
      "Epoch [700/1000], Train Loss: 0.0063, Val Loss: 0.0120\n",
      "Epoch [800/1000], Train Loss: 0.0059, Val Loss: 0.0116\n",
      "Epoch [900/1000], Train Loss: 0.0056, Val Loss: 0.0113\n",
      "Epoch [1000/1000], Train Loss: 0.0054, Val Loss: 0.0110\n",
      "Model is trained!!!\n",
      "Ready to predict the repo trend...\n",
      "Time taken to train the model 0.6431636810302734\n",
      "Trend predictor running ...\n",
      "Predictions are :  [[166494.38]\n",
      " [192585.62]\n",
      " [181395.83]\n",
      " [173879.81]\n",
      " [121135.32]\n",
      " [128242.16]\n",
      " [127095.96]\n",
      " [ 95084.86]\n",
      " [120118.57]\n",
      " [ 89371.47]]\n",
      "Total time :  0.6661150455474854\n",
      "Waiting for next file...\n",
      "New CSV detected: /Users/shivamraj/Desktop/532/532_project/Github-repo-trend-predictor/kafka_streaming/csv_data/stream_2024-12-07_01-48-12.csv\n",
      "Processing CSV file ...\n",
      "pre-processing the data...\n",
      "Trend predictor running ...\n",
      "Predictions are :  [[-6383.226 ]\n",
      " [-6919.8765]\n",
      " [-5555.279 ]\n",
      " [-4650.8984]\n",
      " [-6158.8423]\n",
      " [-7070.7993]\n",
      " [-7101.9297]\n",
      " [-7625.003 ]\n",
      " [-7027.0376]\n",
      " [-7880.9673]]\n",
      "Total time :  0.08561396598815918\n",
      "Waiting for next file...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from watchdog.observers import Observer\n",
    "from watchdog.events import FileSystemEventHandler\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error  # for regression tasks\n",
    "import numpy as np\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 32)  # Assuming 3 input features\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, 32) \n",
    "        self.fc4 = nn.Linear(32, 1) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  # ReLU activation for hidden layers\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CSVHandler(FileSystemEventHandler):\n",
    "    def __init__(self, directory):\n",
    "        self.directory = directory\n",
    "        self.processed_count = 0\n",
    "        self.train_file = \"train.csv\"\n",
    "        self.val_file = \"validation.csv\"\n",
    "        self.test_file = \"test.csv\"\n",
    "\n",
    "    def on_created(self, event):\n",
    "        if event.src_path.endswith('.csv'):\n",
    "            print(f\"New CSV detected: {event.src_path}\")\n",
    "            self.process_csv(event.src_path)\n",
    "\n",
    "    def process_csv(self, file_path):\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            # Read the CSV file\n",
    "            data = pd.read_csv(file_path)\n",
    "            print(f\"Processing CSV file ...\")  # Example: print first 5 rows\n",
    "\n",
    "            #initialise the model\n",
    "            model = SimpleNN()\n",
    "\n",
    "            #preprocess the data\n",
    "            print(\"pre-processing the data...\")\n",
    "            dataset = self.pre_process(data)\n",
    "\n",
    "\n",
    "            if self.processed_count == 0:\n",
    "                print(f\"Processing to get unique 400 data...\")  # Custom logic for the first 3 files\n",
    "                self.add_to_train(dataset)\n",
    "\n",
    "\n",
    "            if self.processed_count == 1:\n",
    "                print(\"Creating validation dataset...\")\n",
    "                self.add_to_validation(dataset)\n",
    "\n",
    "\n",
    "            if self.processed_count == 2:\n",
    "                training_start_time = time.time()\n",
    "                print(\"Getting training data and validation data and normalising...\")\n",
    "                train_data = pd.read_csv(self.train_file)\n",
    "                validation_data = pd.read_csv(self.val_file)\n",
    "\n",
    "                train_len = len(train_data)\n",
    "\n",
    "                print(\"Combine the data for normalization...\")\n",
    "                combined_data = pd.concat([train_data, validation_data])\n",
    "\n",
    "                print(\"Normalising...\")\n",
    "                combined_data = self.normalise(combined_data)\n",
    "\n",
    "                n_train = combined_data.iloc[:train_len].reset_index(drop=True)\n",
    "                n_validation = combined_data.iloc[train_len:].reset_index(drop=True)\n",
    "\n",
    "                print(\"Training the model...\")\n",
    "                self.train_the_model(n_train,n_validation, model)\n",
    "                self.processed_count = 3\n",
    "                training_end_time = time.time()\n",
    "                print(\"Model is trained!!!\")\n",
    "                print(\"Ready to predict the repo trend...\")\n",
    "                print(\"Time taken to train the model\",  training_end_time - training_start_time)\n",
    "            \n",
    "            if self.processed_count == 3:\n",
    "                print(\"Trend predictor running ...\")\n",
    "                self.add_to_test(dataset)\n",
    "                popularity_score = self.predict_the_trend(model)\n",
    "                self.sort_by_popularity(popularity_score)\n",
    "\n",
    "            end_time = time.time()\n",
    "            print(\"Total time : \", end_time - start_time)\n",
    "            print(\"Waiting for next file...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process : {e}\")\n",
    "\n",
    "    def sort_by_popularity(self, popularity_score):\n",
    "        test_file = \"test.csv\"\n",
    "        test_data = pd.read_csv(test_file)\n",
    "\n",
    "        test_data['popularity_score'] = popularity_score\n",
    "        sorted_data = test_data.sort_values(by='popularity_score', ascending=False)\n",
    "\n",
    "        sorted_data.to_csv(\"sorted_test.csv\", index=False)\n",
    "        return\n",
    "\n",
    "    def pre_process(self, dataset):\n",
    "        dataset['Description'] = dataset['Description'].fillna('')\n",
    "        dataset['description_length'] = dataset['Description'].apply(len)\n",
    "\n",
    "        columns_to_drop = ['FullName', 'HtmlUrl', 'Description', 'Language','OwnerLogin', 'OwnerType', 'CreatedAt', 'UpdatedAt', 'PushedAt', 'License','WatchersCount']\n",
    "\n",
    "        dataset = dataset.drop(columns=columns_to_drop)\n",
    "\n",
    "        return dataset\n",
    "    \n",
    "\n",
    "    \n",
    "    def add_to_train(self, dataset):\n",
    "        try:\n",
    "            if os.path.exists(self.train_file):\n",
    "                train_data = pd.read_csv(self.train_file)\n",
    "                print(\"extracting from train.csv...\")\n",
    "            else:\n",
    "                train_data = pd.DataFrame()\n",
    "\n",
    "            combined_data = pd.concat([train_data, dataset]).drop_duplicates()\n",
    "\n",
    "            combined_data.to_csv(self.train_file, index=False)\n",
    "            print(f\"Length of train.csv {len(combined_data)}\")\n",
    "\n",
    "            # Limit to 400 unique rows\n",
    "            if len(combined_data) > 200:\n",
    "                combined_data = combined_data.iloc[:400]\n",
    "                self.processed_count = 1\n",
    "                print(\"Train.csv has reached its maximum capacity of 400 unique rows.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error updating {self.train_file}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "    def add_to_validation(self,dataset):\n",
    "        try:\n",
    "            if os.path.exists(self.val_file):\n",
    "                val_data = pd.read_csv(self.val_file)\n",
    "            else:\n",
    "                val_data = pd.DataFrame()\n",
    "\n",
    "            combined_data = pd.concat([val_data, dataset]).drop_duplicates()\n",
    "            combined_data.to_csv(self.val_file, index=False)\n",
    "            print(f\"length of validation.csv {len(combined_data)}\")\n",
    "\n",
    "            # Limit to 100 unique rows\n",
    "            if len(combined_data) > 100:\n",
    "                combined_data = combined_data.iloc[:100]\n",
    "                self.processed_count = 2\n",
    "                print(\"Validation.csv has reached its maximum capacity of 100 unique rows.\")\n",
    "               \n",
    "        except Exception as e:\n",
    "            print(f\"Error updating {self.val_file}: {e}\")\n",
    "\n",
    "    def add_to_test(self,dataset):\n",
    "        try:\n",
    "            test_data = pd.DataFrame()\n",
    "\n",
    "            combined_data = pd.concat([test_data, dataset]).drop_duplicates()\n",
    "            combined_data.to_csv(self.test_file, index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error updating {self.test_file}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "    def train_the_model(self, train_dataset, validation_dataset, model):\n",
    "        try:\n",
    "            scaler = MinMaxScaler()\n",
    "\n",
    "            #Tranform training data for model\n",
    "            X_train = scaler.fit_transform(train_dataset[['OpenIssuesCount', 'ForksCount', 'Size', 'description_length']])\n",
    "            y_train = train_dataset['StargazersCount'].values\n",
    "\n",
    "            X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "            y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "            y_train_scaled = scaler.fit_transform(y_train.reshape(-1, 1))\n",
    "\n",
    "            #Transform validation data for model\n",
    "            X_val = scaler.fit_transform(validation_dataset[['OpenIssuesCount', 'ForksCount', 'Size', 'description_length']])\n",
    "            y_val = validation_dataset['StargazersCount'].values\n",
    "\n",
    "            X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "            y_val = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "            y_val_scaled = scaler.fit_transform(y_val.reshape(-1, 1))\n",
    "\n",
    "            #model\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "            \n",
    "            self.train(model, X_train, y_train_scaled, X_val, y_val_scaled, criterion, optimizer, 1000)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in Model training :( {e}\")\n",
    "        return\n",
    "    \n",
    "    def predict_the_trend(self,model):\n",
    "        try:\n",
    "            scaler = MinMaxScaler()\n",
    "\n",
    "            test_data = pd.read_csv(self.test_file)\n",
    "            n_test = self.normalise(test_data)\n",
    "\n",
    "            #Transform testing data for model\n",
    "            X_test = scaler.fit_transform(n_test[['OpenIssuesCount', 'ForksCount', 'Size', 'description_length']])\n",
    "            y_test = n_test['StargazersCount'].values\n",
    "\n",
    "            X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "            y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "            y_test = scaler.fit_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "            y_test_pred_scaled = model(X_test)\n",
    "\n",
    "            y_test_pred  = scaler.inverse_transform(y_test_pred_scaled.detach().numpy())\n",
    "            print(\"Predictions are : \",y_test_pred[:10])\n",
    "            return y_test_pred\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in predicting the trend {e}\")\n",
    "\n",
    "    def normalise(self, dataset):\n",
    "        columns_to_normalize = ['OpenIssuesCount', 'ForksCount', 'Size', 'description_length']\n",
    "        data_to_normalize = dataset[columns_to_normalize]\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "        normalized_data = scaler.fit_transform(data_to_normalize)\n",
    "\n",
    "        dataset[columns_to_normalize] = normalized_data\n",
    "\n",
    "        return dataset\n",
    "    \n",
    "\n",
    "    def train(self, model, X_train, y_train, X_val, y_val, criterion, optimizer, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            \n",
    "            # Convert to PyTorch tensors\n",
    "            X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "            y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)  # Reshape to column vector\n",
    "            \n",
    "            optimizer.zero_grad()  # Zero gradients\n",
    "            predictions = model(X_train_tensor)  # Forward pass\n",
    "            loss = criterion(predictions, y_train_tensor)  # Compute loss\n",
    "            \n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "            \n",
    "            # Validation step\n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "                    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
    "                    val_predictions = model(X_val_tensor)\n",
    "                    val_loss = criterion(val_predictions, y_val_tensor)\n",
    "\n",
    "                # Print losses for each epoch\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    watch_directory = \"./kafka_streaming/csv_data\"  # Specify your directory containing CSV files\n",
    "\n",
    "    if not os.path.exists(watch_directory):\n",
    "        os.makedirs(watch_directory)\n",
    "\n",
    "    event_handler = CSVHandler(watch_directory)\n",
    "    observer = Observer()\n",
    "    observer.schedule(event_handler, watch_directory, recursive=False)\n",
    "\n",
    "    print(f\"Monitoring directory: {watch_directory} for new CSV files...\")\n",
    "    try:\n",
    "        observer.start()\n",
    "        while True:\n",
    "            time.sleep(5)  # Keeps the script running\n",
    "    except KeyboardInterrupt:\n",
    "        observer.stop()\n",
    "    observer.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   OpenIssuesCount  ForksCount  StargazersCount      Size  description_length\n",
      "0         0.001405    0.323685           331707  0.000062                  53\n",
      "1         0.012317    0.393350           316814  0.000204                  30\n",
      "2         0.011043    0.333945           306876  0.000043                  73\n",
      "3         0.001797    0.899674           306451  0.000905                  69\n",
      "4         0.001340    0.455732           296377  0.009895                 100\n"
     ]
    }
   ],
   "source": [
    "#normalise\n",
    "\n",
    "\n",
    "\n",
    "columns_to_normalize = ['OpenIssuesCount', 'ForksCount', 'Size', 'description_length']\n",
    "data_to_normalize = dataset[columns_to_normalize]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(data_to_normalize)\n",
    "\n",
    "dataset[columns_to_normalize] = normalized_data\n",
    "\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.16920966e-01 2.03640300e-01 1.53213288e-02 4.62724936e-03]\n",
      " [9.18090633e-03 1.36811267e-01 7.17448832e-03 3.42759212e-03]\n",
      " [8.03737707e-03 1.09067806e-02 1.97009079e-04 1.74807198e-02]\n",
      " [5.47913876e-02 5.25219724e-02 9.55433205e-03 9.42587832e-03]\n",
      " [2.71179795e-03 2.26724554e-02 4.05129828e-05 6.34104542e-03]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X = scaler.fit_transform(dataset[['OpenIssuesCount', 'ForksCount', 'Size', 'description_length']])\n",
    "Y = dataset['StargazersCount'].values\n",
    "\n",
    "\n",
    "X_train, X_temp, y_train, y_temp, ids_train, ids_temp = train_test_split( X, Y, ids, test_size=0.4, random_state=42)\n",
    "\n",
    "\n",
    "X_val, X_test, y_val, y_test, ids_val, ids_test = train_test_split(X_temp, y_temp, ids_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(X_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_target = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on the training target and transform it\n",
    "y_train_scaled = scaler_target.fit_transform(y_train.reshape(-1, 1))\n",
    "\n",
    "# Apply the same transformation to the validation set\n",
    "y_val_scaled = scaler_target.transform(y_val.reshape(-1, 1))\n",
    "\n",
    "y_test_scaled = scaler_target.transform(y_test.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.1692e-01, 2.0364e-01, 1.5321e-02, 4.6272e-03],\n",
      "        [9.1809e-03, 1.3681e-01, 7.1745e-03, 3.4276e-03],\n",
      "        [8.0374e-03, 1.0907e-02, 1.9701e-04, 1.7481e-02],\n",
      "        [5.4791e-02, 5.2522e-02, 9.5543e-03, 9.4259e-03],\n",
      "        [2.7118e-03, 2.2672e-02, 4.0513e-05, 6.3410e-03]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error  # for regression tasks\n",
    "import numpy as np\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)  # reshape for single output\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "print(X_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNN()\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, y_train, X_val, y_val, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)  # Reshape to column vector\n",
    "        \n",
    "        optimizer.zero_grad()  # Zero gradients\n",
    "        predictions = model(X_train_tensor)  # Forward pass\n",
    "        loss = criterion(predictions, y_train_tensor)  # Compute loss\n",
    "        \n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        # Validation step\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "                y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
    "                val_predictions = model(X_val_tensor)\n",
    "                val_loss = criterion(val_predictions, y_val_tensor)\n",
    "\n",
    "            # Print losses for each epoch\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8r/vk9p_0jn559ggvy1pwy_gczm0000gn/T/ipykernel_32377/47673160.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
      "/var/folders/8r/vk9p_0jn559ggvy1pwy_gczm0000gn/T/ipykernel_32377/47673160.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Train Loss: 0.0102, Val Loss: 0.0132\n",
      "Epoch [200/1000], Train Loss: 0.0099, Val Loss: 0.0127\n",
      "Epoch [300/1000], Train Loss: 0.0093, Val Loss: 0.0121\n",
      "Epoch [400/1000], Train Loss: 0.0083, Val Loss: 0.0109\n",
      "Epoch [500/1000], Train Loss: 0.0070, Val Loss: 0.0091\n",
      "Epoch [600/1000], Train Loss: 0.0058, Val Loss: 0.0074\n",
      "Epoch [700/1000], Train Loss: 0.0053, Val Loss: 0.0064\n",
      "Epoch [800/1000], Train Loss: 0.0051, Val Loss: 0.0062\n",
      "Epoch [900/1000], Train Loss: 0.0049, Val Loss: 0.0062\n",
      "Epoch [1000/1000], Train Loss: 0.0048, Val Loss: 0.0063\n"
     ]
    }
   ],
   "source": [
    "train(model, X_train, y_train_scaled, X_val, y_val_scaled, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8r/vk9p_0jn559ggvy1pwy_gczm0000gn/T/ipykernel_32377/813048677.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have test data X_test\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_test_pred_scaled = model(X_test_tensor)\n",
    "    \n",
    "# Convert the predictions back to the original scale of the target variable\n",
    "y_test_pred = scaler_target.inverse_transform(y_test_pred_scaled.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 32586.96 ]\n",
      " [ 26454.213]\n",
      " [ 32164.516]\n",
      " [147362.56 ]\n",
      " [ 33257.246]\n",
      " [ 25545.281]\n",
      " [ 50125.402]\n",
      " [ 30236.158]\n",
      " [ 26890.11 ]\n",
      " [ 30572.23 ]]\n",
      "tensor([[21064.],\n",
      "        [23680.],\n",
      "        [25810.],\n",
      "        [78940.],\n",
      "        [45386.],\n",
      "        [32945.],\n",
      "        [48390.],\n",
      "        [22882.],\n",
      "        [22688.],\n",
      "        [23864.]])\n"
     ]
    }
   ],
   "source": [
    "print(y_test_pred[:10])\n",
    "print(y_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 13733.9590\n",
      "Mean Squared Error (MSE): 846656640.0000\n",
      "Root Mean Squared Error (RMSE): 29097.3652\n",
      "R-squared (R²): 0.4309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8r/vk9p_0jn559ggvy1pwy_gczm0000gn/T/ipykernel_32377/1461250871.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Assuming the model is already trained, and you've made predictions on the test set (X_test)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_test_pred_scaled = model(X_test_tensor)\n",
    "\n",
    "# Inverse transform the predictions back to original scale if you normalized the target\n",
    "y_test_pred = scaler_target.inverse_transform(y_test_pred_scaled.detach().numpy())\n",
    "\n",
    "# # If you normalized your test labels as well, do the same for y_test\n",
    "# y_test_original = scaler_target.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Calculate the metrics\n",
    "mae = mean_absolute_error(y_test, y_test_pred)\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "rmse = mean_squared_error(y_test, y_test_pred, squared=False)  # RMSE\n",
    "r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Print out the metrics\n",
    "print(f'Mean Absolute Error (MAE): {mae:.4f}')\n",
    "print(f'Mean Squared Error (MSE): {mse:.4f}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse:.4f}')\n",
    "print(f'R-squared (R²): {r2:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
